{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37389cf2-b0bb-4f56-a0b9-52a66b35fb6a",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "**Note**: \n",
    "* Runing the following commands in the main folder of the repository (where the README.md and .py files are located)\n",
    "* For visualization of the spectrogram and model predictions, run the corresponding code in jupyter notebook.\n",
    "\n",
    "## Evaluate the model on a dataset using the frame-wise F1 score ($F1_\\text{frame}$) and segment-wise F1 score ($F1_\\text{seg}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07351458-ac82-4e62-8147-94d42261eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35185275-b86d-4ba4-b04e-63e305f4b980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.13s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'segment_wise_scores': {'N-true-positive': 124,\n",
       "  'N-positive-in-prediction': 125,\n",
       "  'N-positive-in-ground-truth': 133,\n",
       "  'precision': 0.992,\n",
       "  'recall': 0.9323308270676691,\n",
       "  'F1': 0.9612403100775192},\n",
       " 'frame_wise_scores': {'N-true-positive': 5042,\n",
       "  'N-positive-in-prediction': 5105,\n",
       "  'N-positive-in-ground-truth': 5207,\n",
       "  'precision': 0.9876591576885406,\n",
       "  'recall': 0.9683118878432879,\n",
       "  'F1': 0.9778898370830101}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_dataset( \"data/example_subset/Mouse/test/\", \"nccratliri/whisperseg-large-ms-ct2\", num_trials =3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf54498d-2bd9-4be2-b01f-6f8c8725738d",
   "metadata": {},
   "source": [
    "## Evaluate the model by visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf7fa97d-a1a6-48b8-991f-fed2adf2a581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from model import WhisperSegmenterFast\n",
    "from audio_utils import SpecViewer\n",
    "import librosa\n",
    "import json\n",
    "segmenter = WhisperSegmenterFast( \"nccratliri/whisperseg-large-ms-ct2\", device=\"cuda\" )\n",
    "spec_viewer = SpecViewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "825144c9-5252-40f9-86d1-f64a18c1cdec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d17e023b2a4402aa779d287db53e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='offset', max=0.0, step=0.75), Output()), _dom_classe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function ipywidgets.widgets.interaction._InteractFactory.__call__.<locals>.<lambda>(*args, **kwargs)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr = 32000  \n",
    "min_frequency = 0\n",
    "spec_time_step = 0.0025\n",
    "min_segment_length = 0.01\n",
    "eps = 0.02\n",
    "num_trials = 3\n",
    "\n",
    "audio_file = \"data/example_subset/Zebra_finch/test_juveniles/zebra_finch_R3428_40932.29996086_1_24_8_19_56.wav\"\n",
    "label_file = audio_file[:-4] + \".json\"\n",
    "audio, _ = librosa.load( audio_file, sr = sr )\n",
    "label = json.load( open(label_file) )\n",
    "\n",
    "prediction = segmenter.segment(  audio, sr = sr, min_frequency = min_frequency, spec_time_step = spec_time_step,\n",
    "                       min_segment_length = min_segment_length, eps = eps,num_trials = num_trials )\n",
    "spec_viewer.visualize( audio = audio, sr = sr, min_frequency= min_frequency, prediction = prediction, label=label, \n",
    "                       window_size=15, precision_bits=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7378172-10b5-4d96-b28a-d149a32f45eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wseg]",
   "language": "python",
   "name": "conda-env-wseg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
