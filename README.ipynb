{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72fef482-a129-44bc-a99c-da32d50b1347",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d3251-b38e-4a75-887b-4f60bfbe7179",
   "metadata": {},
   "source": [
    "## Option 1: Set up environment in your local computer\n",
    "1. Install Miniconda (or Anaconda)\n",
    "https://docs.conda.io/en/latest/miniconda.html\n",
    "\n",
    "2. In the \"base\" anaconda environment, create a new environment \"syllable_segment\" (This installation will take a few minutes.):\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "3. Activate the anaconda environment \"syllable_segment\":\n",
    "```bash\n",
    "conda activate syllable_segment\n",
    "```\n",
    "Alternatively, one can go through all the packages that are imported, and install the missing packages manually.\n",
    "\n",
    "## Option 2: Run the code on google colab (with GPU runtime)\n",
    "Since google colab has preinstalled most of the package, we only need to install the \"transformers\" package\n",
    "```bash\n",
    "pip install transformers\n",
    "```\n",
    "\n",
    "The following commands assume that this jupyter notebook is running within the created anaconda environment, or with all necessary python packages installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3956eac2-46a0-4ed6-bc7b-2c70eec38934",
   "metadata": {},
   "source": [
    "# Use the pretrained WhisperSeg in command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d46d8c00-bae9-45fc-a7fe-bb697f11f225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model import WhisperSegmenter\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a2c0e4-c69e-4ae8-b3a5-deae090522c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the segmenter\n",
    "segmenter = WhisperSegmenter(  model_path = \"nianlong/vocal-segment-zebra-finch-whisper-large\", \n",
    "                        device = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2703150-28a4-4bed-ac98-692e23ea9fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load an audio file, and resample the audio to the sampling rate 16000 Hz \n",
    "audio_file_name = \"data/R3406_035/test/R3406_40911.54676404_1_3_15_11_16.wav\"\n",
    "audio, _ = librosa.load( audio_file_name, sr = 16000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46784449-5eb6-4ee4-afe0-75ada540b8d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# segment the audio, i.e., predict the paired on/offset of the audio\n",
    "prediction = segmenter.segment( audio )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e498890-5775-457b-80a8-6ca24044cef8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'onset': array([1.47, 1.83, 1.93, 2.06, 2.17, 2.26, 2.43, 2.72, 2.92, 3.06, 3.14,\n",
       "        3.28, 3.62, 3.82, 4.1 , 4.2 , 4.97]),\n",
       " 'offset': array([1.58, 1.88, 2.03, 2.1 , 2.24, 2.37, 2.58, 2.84, 3.03, 3.13, 3.24,\n",
       "        3.43, 3.68, 3.9 , 4.15, 4.39, 5.07])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67504c22-ed57-433e-b63f-6d07ef750205",
   "metadata": {},
   "source": [
    "To save the prediction into a .csv file, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43cf0f3b-f790-4f30-b4d3-e476d28b6a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(prediction).to_csv(\"predicted_annotations.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38482d13-59b2-4c90-af0f-278afe6ddccb",
   "metadata": {},
   "source": [
    "# Visualize the prediction\n",
    "This visualize function is only supported on jupter notebook and google colab, because it is an interactive plot that is a ipywidget feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dbd82b1-d160-417f-bf0b-d4b8e1e765b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67f2c7adda54730827ec8d6532f6e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='offset', max=2.0530625000000002), Output()), _dom_cl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function ipywidgets.widgets.interaction._InteractFactory.__call__.<locals>.<lambda>(*args, **kwargs)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmenter.visualize( audio = audio, prediction = prediction, audio_file_name = audio_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82676c41-b0ba-46f5-8b68-36244784cdaa",
   "metadata": {},
   "source": [
    "If we know the ground-truth label, we can also plot both the predicted label and the ground-truth label to visualize the prediction error.\n",
    "\n",
    "For example, we have the annotation file for the following wav file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a18e63c5-6b14-4d03-921e-2d8cfcba88ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/R3406_035/test/R3406_40911.54676404_1_3_15_11_16.wav',\n",
       " 'data/R3406_035/test/R3406_40911.54676404_1_3_15_11_16.csv')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_annotation_file_name = audio_file_name[:-4]+\".csv\"\n",
    "audio_file_name, human_annotation_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6819a71-7c73-4053-811d-ff1f0ed3ebb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Both label and prediction is a dictionary. \n",
    "The dictionary contains two keys: onset and offset. \n",
    "The value for each key is an numpy array\n",
    "\"\"\"\n",
    "label_df = pd.read_csv( human_annotation_file_name )\n",
    "label = {\n",
    "    \"onset\":np.array(label_df[\"onset\"]),\n",
    "    \"offset\":np.array(label_df[\"offset\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a60b0398-9fca-422c-888d-308da64c83f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27233042b13a40258b7b7df271c8e615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='offset', max=2.0530625000000002), Output()), _dom_cl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function ipywidgets.widgets.interaction._InteractFactory.__call__.<locals>.<lambda>(*args, **kwargs)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmenter.visualize( audio = audio, prediction = prediction, label = label, audio_file_name = audio_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35698f3f-979c-47f5-a7cc-a501fe2357dc",
   "metadata": {},
   "source": [
    "# Finetune WhisperSeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b70a50-78f1-4500-afe9-b849e8207f9a",
   "metadata": {},
   "source": [
    "We are going to finetune WhisperSeg on the zebra finch dataset released by the DAS paper. In this dataset the researchers have adopted a very different standard when segmenting the syllables. So the model \"vocal-segment-zebra-finch-whisper-large\" that was pretrained on Tomas's dataset will perform not well. Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "020186f6-e78b-42b8-9760-b667fd5518d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1030cb6a0ae4ab19963439a456f35f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=4.3, description='offset', max=8.6498125), Output()), _dom_classes=('w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function ipywidgets.widgets.interaction._InteractFactory.__call__.<locals>.<lambda>(*args, **kwargs)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_file_name = \"data/DAS_zebra_finch/test/birdname_130519_113316.31.wav\"\n",
    "human_annotation_file_name = \"data/DAS_zebra_finch/test/birdname_130519_113316.31.csv\"\n",
    "audio, _ = librosa.load( audio_file_name, sr = 16000 )\n",
    "label_df = pd.read_csv( human_annotation_file_name )\n",
    "label = {\n",
    "    \"onset\":np.array(label_df[\"onset\"]),\n",
    "    \"offset\":np.array(label_df[\"offset\"])\n",
    "}\n",
    "prediction = segmenter.segment( audio )\n",
    "segmenter.visualize( audio = audio, prediction = prediction, label = label, audio_file_name = audio_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f2df2-eb14-41bc-8eb9-b295a2d7033c",
   "metadata": {},
   "source": [
    "There are quite a lot of False Positives! That's why we need to finetune WhisperSeg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078345cb-5bf6-4ee3-8489-ee80bce0572d",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "**Before finetuning WhisperSeg, we need to first prepare the training dataset and the test dataset.**\n",
    "\n",
    "Take the training dataset as an example: \n",
    "* All training audio and annotation files should be placed in the same folder.\n",
    "* The audio file should have a format \".wav\" (lowercase),  and the annotation has a format \".csv\"\n",
    "* The names of the audio file and the corresponding annotation file should be matched. For example, if there is an audio file named \"XXXXX_bird_12345.wav\", the corresponding annotation file needs to be named as \"XXXXX_bird_12345.csv\"\n",
    "* Inside the annotation file, there will be two columns: \"onset\" and \"offset\". The unit of the value is second.\n",
    "* The .wav file can have various sampling rate. The model will resample them to 16kHz automatically.\n",
    "\n",
    "For the testing dataset, the requirement is the same.\n",
    "\n",
    "Please check the folder: data/DAS_zebra_finch/ for concrete examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4171ca6-0b6b-4f98-bd06-8acbb24fcd3e",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Note: Before runing the following command, it is recommended to restart this jupyter notebook by Kernel -> Restart Kernel. This will release the GPU memory used in the previous cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e0c9ca-2485-432f-aaa8-1a3b409fd6c8",
   "metadata": {},
   "source": [
    "Explanation of the training parameters:\n",
    "* initial_model_path: The initail checkpoint of Wshiper, here we use the whisper model pretrained on Tomas's dataset\n",
    "* model_folder: the folder to save the trained checkpoint\n",
    "* result_folder: the folder to save some loging information and validation and test results\n",
    "* train_dataset_folder: the folder that contains all the paired training audio and annotation data as described above\n",
    "* test_dataset_folder: \n",
    "* warmup_steps: the learning rate will increase from 0 linearly to 1e-6 within warmup steps\n",
    "* save_every: save the checkpoint after save_every training steps\n",
    "* max_num_iterations: the maximum number of training steps before the training finishes.\n",
    "* batch_size: Training WhisperSeg requires around 40 GB GPU RAM if we use a batch size of 4. For smaller GPU, please try batch size 2 or 1.\n",
    "\n",
    "Since the DAS_zebra_finch is a very small dataset, we finetune WhisperSeg for 1000 steps, and set up the warmup step to 200. \n",
    "We do not create validation set, and use all the training set to train the model until the max_num_iterations is reached, and we only keep the model checkpont at the max_num_iterations. Empirically this works very stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a24e0ee-3d98-499a-bf18-8b1f34520f95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/meilong/miniconda3/envs/syllable_segment/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]/home/meilong/miniconda3/envs/syllable_segment/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "100%|███████████████████████████████████████████| 13/13 [00:08<00:00,  1.53it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.14it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.13it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.77it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.14it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.10it/s]\n",
      " 62%|███████████████████████████                 | 8/13 [00:03<00:02,  2.21it/s]Epoch: 7, current_batch: 100, learning rate: 0.000000, Loss: 1.1610\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.13it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.11it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.12it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.14it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  1.99it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.85it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  1.98it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.13it/s]\n",
      " 31%|█████████████▌                              | 4/13 [00:02<00:04,  2.08it/s]Epoch: 15, current_batch: 200, learning rate: 0.000001, Loss: 0.2999\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.14it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.14it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.10it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.11it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.11it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.13it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.78it/s]\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]Epoch: 23, current_batch: 300, learning rate: 0.000001, Loss: 0.1857\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.13it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.10it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.11it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.11it/s]\n",
      " 69%|██████████████████████████████▍             | 9/13 [00:04<00:01,  2.19it/s]Epoch: 30, current_batch: 400, learning rate: 0.000001, Loss: 0.1646\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  1.99it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.84it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.13it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.12it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.10it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.10it/s]\n",
      " 38%|████████████████▉                           | 5/13 [00:02<00:03,  2.09it/s]Epoch: 38, current_batch: 500, learning rate: 0.000001, Loss: 0.1659\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.11it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.75it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.14it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.09it/s]\n",
      "  8%|███▍                                        | 1/13 [00:00<00:08,  1.37it/s]Epoch: 46, current_batch: 600, learning rate: 0.000000, Loss: 0.1448\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.13it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.13it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.10it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.11it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.05it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.80it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.11it/s]\n",
      " 77%|█████████████████████████████████          | 10/13 [00:04<00:01,  2.22it/s]Epoch: 53, current_batch: 700, learning rate: 0.000000, Loss: 0.1405\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.10it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.13it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.10it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.12it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.13it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.11it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.76it/s]\n",
      " 46%|████████████████████▎                       | 6/13 [00:03<00:03,  2.12it/s]Epoch: 61, current_batch: 800, learning rate: 0.000000, Loss: 0.1289\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.10it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.10it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.07it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.11it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.12it/s]\n",
      " 15%|██████▊                                     | 2/13 [00:01<00:06,  1.73it/s]Epoch: 69, current_batch: 900, learning rate: 0.000000, Loss: 0.1262\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.11it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:07<00:00,  1.75it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.13it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.11it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.14it/s]\n",
      " 85%|████████████████████████████████████▍      | 11/13 [00:05<00:00,  2.24it/s]Epoch: 76, current_batch: 1000, learning rate: 0.000000, Loss: 0.1220\n",
      " 85%|████████████████████████████████████▍      | 11/13 [00:12<00:02,  1.16s/it]\n",
      "The best checkpoint on validation set is: model/DAS_zebra_finch/checkpoint-1000,\n",
      "Reporting test results ...\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:06<00:00,  3.23s/it]\n",
      "Test performance: f1 score: 0.9623\n",
      "Removing sub-optimal checkpoints ...\n",
      "All Done!\n"
     ]
    }
   ],
   "source": [
    "!python train.py -initial_model_path nianlong/vocal-segment-zebra-finch-whisper-large -model_folder model/DAS_zebra_finch -result_folder result/DAS_zebra_finch -train_dataset_folder data/DAS_zebra_finch/train -test_dataset_folder data/DAS_zebra_finch/test -warmup_steps 200 -save_every 1000 -max_num_iterations 1000 -batch_size 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b381b9f-6050-4f31-afaa-add06cd8f548",
   "metadata": {},
   "source": [
    "Let's use the finetuned WhisperSeg to segment the audio from the DAS_zebra_finch test set again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c6a3ba0-f564-4c67-bc0d-8546a3ffd685",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048957c939d04f17b3c372545cae5b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=4.3, description='offset', max=8.6498125), Output()), _dom_classes=('w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function ipywidgets.widgets.interaction._InteractFactory.__call__.<locals>.<lambda>(*args, **kwargs)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import WhisperSegmenter\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "segmenter = WhisperSegmenter(  model_path = \"model/DAS_zebra_finch/checkpoint-1000\", \n",
    "                        device = \"cuda\")\n",
    "\n",
    "audio_file_name = \"data/DAS_zebra_finch/test/birdname_130519_113316.31.wav\"\n",
    "human_annotation_file_name = \"data/DAS_zebra_finch/test/birdname_130519_113316.31.csv\"\n",
    "audio, _ = librosa.load( audio_file_name, sr = 16000 )\n",
    "label_df = pd.read_csv( human_annotation_file_name )\n",
    "label = {\n",
    "    \"onset\":np.array(label_df[\"onset\"]),\n",
    "    \"offset\":np.array(label_df[\"offset\"])\n",
    "}\n",
    "prediction = segmenter.segment( audio )\n",
    "segmenter.visualize( audio = audio, prediction = prediction, label = label, audio_file_name = audio_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2730a6-5d28-477c-b742-0a6d4e0da041",
   "metadata": {},
   "source": [
    "Therefore, WhisperSeg does perform better after finetuning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90443bff-a839-44e7-b8d6-65c2e7a2f6e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Speed Up Inference with ctranslate2 - WhisperSegmenterFast\n",
    "\n",
    "The environment.yml has been updated due to the adding of the ctranslate2 package. \n",
    "\n",
    "**Running the code below does not rely on the previous codes in this notebook. One can restart the kernel before run the following code, to release some GPU usage.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca146d0c-d2cc-400d-b8c0-b8484b28f13b",
   "metadata": {},
   "source": [
    "## convert the huggingface Whisper model to the CTranslate2 model, and store the configuration files of the tokenizer and feature-extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe8b83e-fb1c-4379-8478-439f6a5f1daa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Note: The following cell only needs to be run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0c6df5-5d9b-4ed3-87b0-8ecfbc49d04e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/vocal-segment-zebra-finch-whisper-large-ct2/hf_model/tokenizer_config.json',\n",
       " 'model/vocal-segment-zebra-finch-whisper-large-ct2/hf_model/special_tokens_map.json',\n",
       " 'model/vocal-segment-zebra-finch-whisper-large-ct2/hf_model/vocab.json',\n",
       " 'model/vocal-segment-zebra-finch-whisper-large-ct2/hf_model/merges.txt',\n",
       " 'model/vocal-segment-zebra-finch-whisper-large-ct2/hf_model/normalizer.json',\n",
       " 'model/vocal-segment-zebra-finch-whisper-large-ct2/hf_model/added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer\n",
    "import os\n",
    "## If you have trained model on new dataset, replace this hf_model_path's value with the path to the newly saved checkpoint\n",
    "hf_model_path = \"nianlong/vocal-segment-zebra-finch-whisper-large\"\n",
    "## The path to the folder where the converted ctranslate2 model will be saved. \n",
    "## In the meantime, the configuration files for Tokenizer and FeatureExtractors will also be copied to this folder\n",
    "ct2_model_path = \"model/vocal-segment-zebra-finch-whisper-large-ct2\"\n",
    "\n",
    "assert not os.path.exists(ct2_model_path)\n",
    "\n",
    "os.system( \"ct2-transformers-converter --model %s --output_dir %s\"%( hf_model_path, ct2_model_path ) )\n",
    "## copy the configuration file of the original huggingface model, because it contains some useful hyperparameters\n",
    "hf_hub_download(repo_id=hf_model_path, filename=\"config.json\", local_dir = ct2_model_path+\"/hf_model/\")\n",
    "WhisperFeatureExtractor.from_pretrained( hf_model_path ).save_pretrained( ct2_model_path+\"/hf_model/\" )\n",
    "WhisperTokenizer.from_pretrained(hf_model_path, language = \"english\" ).save_pretrained( ct2_model_path+\"/hf_model/\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db220c2a-6571-4652-b96d-5a8064a7424d",
   "metadata": {},
   "source": [
    "## Use the CTranslate2 Converted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f756d192-d9a7-411c-966b-f05f4e20caf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model import WhisperSegmenterFast\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d365f5ab-9d44-4419-94ff-7e19e11d6036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "segmenter_fast = WhisperSegmenterFast( \"model/vocal-segment-zebra-finch-whisper-large-ct2\", device=\"cuda\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b2bd84-1540-45f4-8ac5-1c9d5401292c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1d45a8f0f146e6883106d6d313862e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='offset', max=2.0530625000000002), Output()), _dom_cl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Length: 7.053063 s\n",
      "Segmentation Time: 3.053034 s\n"
     ]
    }
   ],
   "source": [
    "audio_file_name = \"data/R3406_035/test/R3406_40911.54676404_1_3_15_11_16.wav\"\n",
    "human_annotation_file_name = \"data/R3406_035/test/R3406_40911.54676404_1_3_15_11_16.csv\"\n",
    "audio, _ = librosa.load( audio_file_name, sr = 16000 )\n",
    "label_df = pd.read_csv( human_annotation_file_name )\n",
    "label = {\n",
    "    \"onset\":np.array(label_df[\"onset\"]),\n",
    "    \"offset\":np.array(label_df[\"offset\"])\n",
    "}\n",
    "\n",
    "tic = time.time()\n",
    "prediction = segmenter_fast.segment( audio )\n",
    "tac = time.time()\n",
    "\n",
    "segmenter_fast.visualize( audio = audio, prediction = prediction, label = label, audio_file_name = audio_file_name)\n",
    "\n",
    "print(\"Audio Length: %f s\"%(len(audio)/16000))\n",
    "print(\"Segmentation Time: %f s\"%(tac - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21f8999c-5915-46e6-ab9e-39eea99f8ede",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4479b1ad458949e5aba692e225782575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.6000000000000001, description='offset', max=1.3739375000000003), Out…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Length: 6.373938 s\n",
      "Segmentation Time: 0.221841 s\n"
     ]
    }
   ],
   "source": [
    "audio_file_name = \"data/R3277/R3277_40905.13765404_12_28_3_49_25.wav\"\n",
    "audio, _ = librosa.load( audio_file_name, sr = 16000 )\n",
    "\n",
    "tic = time.time()\n",
    "prediction = segmenter_fast.segment( audio )\n",
    "tac = time.time()\n",
    "\n",
    "segmenter_fast.visualize( audio = audio, prediction = prediction, audio_file_name = audio_file_name)\n",
    "\n",
    "print(\"Audio Length: %f s\"%(len(audio)/16000))\n",
    "print(\"Segmentation Time: %f s\"%(tac - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "875e59e4-bfe0-41c5-b494-fd666a2d079d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b1a7acd37340b7a3bae6de9d1f4a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=13.600000000000001, description='offset', max=27.2641875), Output()), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Length: 32.264187 s\n",
      "Segmentation Time: 1.439877 s\n"
     ]
    }
   ],
   "source": [
    "audio_file_name = \"data/R3277/R3277_40905.38807_12_28_10_46_47.wav\"\n",
    "audio, _ = librosa.load( audio_file_name, sr = 16000 )\n",
    "\n",
    "tic = time.time()\n",
    "prediction = segmenter_fast.segment( audio )\n",
    "tac = time.time()\n",
    "\n",
    "segmenter_fast.visualize( audio = audio, prediction = prediction, audio_file_name = audio_file_name)\n",
    "\n",
    "print(\"Audio Length: %f s\"%(len(audio)/16000))\n",
    "print(\"Segmentation Time: %f s\"%(tac - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1f4536-1f4f-4937-8618-cbf67601dc95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92716664778447bcbd9c90f3be377a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.2, description='offset', max=0.4276875000000002), Output()), _dom_cl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Length: 5.427688 s\n",
      "Segmentation Time: 0.357921 s\n"
     ]
    }
   ],
   "source": [
    "audio_file_name = \"data/R3277/R3277_40905.406363_12_28_11_17_16.wav\"\n",
    "audio, _ = librosa.load( audio_file_name, sr = 16000 )\n",
    "\n",
    "tic = time.time()\n",
    "prediction = segmenter_fast.segment( audio )\n",
    "tac = time.time()\n",
    "\n",
    "segmenter_fast.visualize( audio = audio, prediction = prediction, audio_file_name = audio_file_name)\n",
    "\n",
    "print(\"Audio Length: %f s\"%(len(audio)/16000))\n",
    "print(\"Segmentation Time: %f s\"%(tac - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5052dc5-01ef-409f-a518-1e532f34cdff",
   "metadata": {},
   "source": [
    "## Test the Speed of WhisperSegmenterFast\n",
    "\n",
    "Here we use the WhisperSegmenterFast to segment all audios in the folder \"data/DAS_zebra_finch/train/\", and record the time spent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc85a0dc-1b0b-454c-a391-6596f261ffce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of audio (.wav) files: 14\n",
      "Total length of audio files: 175.500438 s\n"
     ]
    }
   ],
   "source": [
    "audio_list = [ librosa.load(\"data/DAS_zebra_finch/train/\"+fname, sr = 16000)[0] for fname in os.listdir(\"data/DAS_zebra_finch/train/\") if fname.endswith(\".wav\") ]\n",
    "\n",
    "num_of_audio_files = len(audio_list)\n",
    "total_audio_length = sum([ len(audio)/16000  for audio in audio_list ])\n",
    "\n",
    "print(\"Total number of audio (.wav) files:\", num_of_audio_files)\n",
    "print(\"Total length of audio files: %f s\"%(total_audio_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "282682cc-3b65-4157-88be-7362c58258b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 14/14 [00:15<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total segmentation time: 15.939726 s for segmenting 2.93 minutes audio\n",
      "Average segmentation speed: 11.010254 s of audio segmented per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "\n",
    "for audio in tqdm(audio_list):\n",
    "    segmenter_fast.segment(audio)\n",
    "    \n",
    "tac = time.time()\n",
    "print(\"Total segmentation time: %f s for segmenting %.2f minutes audio\"%(tac - tic, total_audio_length/60))\n",
    "print(\"Average segmentation speed: %f s of audio segmented per second\"%( total_audio_length / (tac - tic) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e03903-6664-4915-951d-d5f630fa16a9",
   "metadata": {},
   "source": [
    "Note that the default num_trials is 3, which means one audio file will be segmented three times, each time with a slightly different offset. This helps to improve the segmentation accuracy, but it will slow down the segmentation process. \n",
    "\n",
    "If num_trials is set to 1, the segmentation speed will be improved. However, the segmentation accuracy will be slightly impacted. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc0c17-e5c6-459d-88f0-dd1901152fd5",
   "metadata": {},
   "source": [
    "## Speed Comparison between WhisperSegmenterFast and faster-whisper (https://github.com/guillaumekln/faster-whisper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8173a5f-eb62-4a2c-8469-03e2dc6b60b9",
   "metadata": {},
   "source": [
    "### speed of faster-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf72dd4-c099-4714-b44d-f1740094c6eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2006655db3724483b1c8a681cb602b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## This code comes from the github repo of faster-whisper: \n",
    "## https://github.com/guillaumekln/faster-whisper#transcription\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from faster_whisper import WhisperModel\n",
    "model_size = \"large-v2\"\n",
    "faster_whisper_model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c64ddb56-f16e-493f-a3fd-24b9c1c4fcc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating duration from bitrate, this may be inaccurate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of audio: 13.32 min\n",
      "Segmentation time: 56.542324 s for segmenting 13.32 minutes audio\n"
     ]
    }
   ],
   "source": [
    "audio_name = \"data/speed_test/test_audio.mp3\"\n",
    "audio, _ = librosa.load(audio_name, sr = 16000)\n",
    "total_audio_length = len(audio)/16000\n",
    "print(\"Total length of audio: %.2f min\"%(total_audio_length/60))\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "segments, info  = faster_whisper_model.transcribe(audio_name, beam_size=5)\n",
    "\"\"\"\n",
    "    If you comment out the following two lines, you will witness a 8x speedup. \n",
    "    However, this speed is not useful, beacause the segments above is a generator. To get the real content from it,\n",
    "    one must loop through the generator, and this loop turns out to be slow, but necessary. \n",
    "    Therefore, the following two lines should be counted into the time spent by faster-whisper\n",
    "\"\"\"\n",
    "res = []\n",
    "for segment in segments:\n",
    "    res.append((segment.start, segment.end, segment.text))\n",
    "    \n",
    "tac = time.time()\n",
    "print(\"Segmentation time: %f s for segmenting %.2f minutes audio\"%(tac - tic, total_audio_length/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba682b96-5c26-4962-8808-50aa3693d964",
   "metadata": {
    "tags": []
   },
   "source": [
    "The file data/speed_test/test_audio.mp3 is the same file used in the benchmark in https://github.com/guillaumekln/faster-whisper#benchmark, where the authors reported that it took **54 s** to segment this 13 min audio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93675570-27f6-483a-9346-92a479390fcc",
   "metadata": {},
   "source": [
    "### Speed of WhisperSegmenterFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f153c90-b4d9-428f-b873-b72ace47afa0",
   "metadata": {},
   "source": [
    "For a fair comparison, we let WhisperSegmenterFast segment bird song audio that is also 13 min long. \n",
    "\n",
    "This 13-min birdsong audio is created by merging multiple birdsong audio files.\n",
    "\n",
    "We do not let WhisperSegmenterFast segment data/speed_test/test_audio.mp3 because this .mp3 file contains human talk. In this case WhisperSegmenterFast will extract no birdsong syllables from it, and the segmentation will be very fast and we might overestimate the speed of WhisperSegmenterFast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a06cf03-c5ae-40d6-872b-d40db45f125f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model import WhisperSegmenterFast\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6019ec2b-d44a-4166-93f4-8bb936c18d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "segmenter_fast = WhisperSegmenterFast( \"model/vocal-segment-zebra-finch-whisper-large-ct2\", device=\"cuda\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac9c3c60-c0b6-43f8-897f-bb1a9ecf7194",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of audio: 13.32 min\n",
      "Segmentation time: 12.028466 s for segmenting 13.32 minutes audio\n"
     ]
    }
   ],
   "source": [
    "audio_name = \"data/speed_test/test_birdsong_audio.wav\"\n",
    "audio, _ = librosa.load(audio_name, sr = 16000)\n",
    "total_audio_length = len(audio)/16000\n",
    "print(\"Total length of audio: %.2f min\"%(total_audio_length/60))\n",
    "\n",
    "tic = time.time()\n",
    "audio, _ = librosa.load(audio_name, sr = 16000)\n",
    "prediction = segmenter_fast.segment(audio, num_trials= 1)\n",
    "tac = time.time()\n",
    "print(\"Segmentation time: %f s for segmenting %.2f minutes audio\"%(tac - tic, total_audio_length/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8cbd58-0438-4584-be79-873355a81f0c",
   "metadata": {},
   "source": [
    "The segmentation looks reasonable, as shown by visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d81e25e-7f16-4d8a-83e2-15c47fcca6e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d409e6dbcd644e3a99379d52c898ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=397.1, description='offset', max=794.2), Output()), _dom_classes=('wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function ipywidgets.widgets.interaction._InteractFactory.__call__.<locals>.<lambda>(*args, **kwargs)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmenter_fast.visualize(audio = audio, prediction=prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8bafb7-8e22-4f0d-92ef-22f73a80c2a4",
   "metadata": {},
   "source": [
    "**Conclusion: The speed between both WhisperSegmenterFast and faster-whipser is comparable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c8150b-df4d-4980-8af9-03830398da21",
   "metadata": {},
   "source": [
    "### Speed on CPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "795fd79a-5595-4013-8b3c-43b305cf2332",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a03923ad1d142b082298922bd9d496f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9d6eda01b04c448656f1737a4b72e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)db28b7c373/README.md:   0%|          | 0.00/2.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde130abb31f460c8f9eb85ea7371871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7c373/.gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6194b68e5ffb4273956a73962df25511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7c373/vocabulary.txt:   0%|          | 0.00/460k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c933cb89b414f649ff3a27a6aa9a04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)28b7c373/config.json:   0%|          | 0.00/2.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e2ec8b43b048ecb1cb64c97c13d803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.bin:   0%|          | 0.00/484M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45648af8a4d74aca885f75d72f71aadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7c373/tokenizer.json:   0%|          | 0.00/2.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating duration from bitrate, this may be inaccurate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of audio: 13.32 min\n",
      "Segmentation time: 210.623049 s for segmenting 13.32 minutes audio\n"
     ]
    }
   ],
   "source": [
    "## This code comes from the github repo of faster-whisper: \n",
    "## https://github.com/guillaumekln/faster-whisper#transcription\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from faster_whisper import WhisperModel\n",
    "model_size = \"small\"\n",
    "faster_whisper_model = WhisperModel(model_size, device=\"cpu\", compute_type=\"float32\")\n",
    "\n",
    "audio_name = \"data/speed_test/test_audio.mp3\"\n",
    "audio, _ = librosa.load(audio_name, sr = 16000)\n",
    "total_audio_length = len(audio)/16000\n",
    "print(\"Total length of audio: %.2f min\"%(total_audio_length/60))\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "segments, info  = faster_whisper_model.transcribe(audio_name, beam_size=5)\n",
    "\"\"\"\n",
    "    If you comment out the following two lines, you will witness a 8x speedup. \n",
    "    However, this speed is not useful, beacause the segments above is a generator. To get the real content from it,\n",
    "    one must loop through the generator, and this loop turns out to be slow, but necessary. \n",
    "    Therefore, the following two lines should be counted into the time spent by faster-whisper\n",
    "\"\"\"\n",
    "res = []\n",
    "for segment in segments:\n",
    "    res.append((segment.start, segment.end, segment.text))\n",
    "    \n",
    "tac = time.time()\n",
    "print(\"Segmentation time: %f s for segmenting %.2f minutes audio\"%(tac - tic, total_audio_length/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e80ac8e-e163-4f10-9e18-2feb93c95a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6ac91a9533481b8f648a0e264a4a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating duration from bitrate, this may be inaccurate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of audio: 13.32 min\n",
      "Segmentation time: 1192.557899 s for segmenting 13.32 minutes audio\n"
     ]
    }
   ],
   "source": [
    "## This code comes from the github repo of faster-whisper: \n",
    "## https://github.com/guillaumekln/faster-whisper#transcription\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from faster_whisper import WhisperModel\n",
    "model_size = \"large-v2\"\n",
    "faster_whisper_model = WhisperModel(model_size, device=\"cpu\", compute_type=\"float32\")\n",
    "\n",
    "audio_name = \"data/speed_test/test_audio.mp3\"\n",
    "audio, _ = librosa.load(audio_name, sr = 16000)\n",
    "total_audio_length = len(audio)/16000\n",
    "print(\"Total length of audio: %.2f min\"%(total_audio_length/60))\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "segments, info  = faster_whisper_model.transcribe(audio_name, beam_size=5)\n",
    "\"\"\"\n",
    "    If you comment out the following two lines, you will witness a 8x speedup. \n",
    "    However, this speed is not useful, beacause the segments above is a generator. To get the real content from it,\n",
    "    one must loop through the generator, and this loop turns out to be slow, but necessary. \n",
    "    Therefore, the following two lines should be counted into the time spent by faster-whisper\n",
    "\"\"\"\n",
    "res = []\n",
    "for segment in segments:\n",
    "    res.append((segment.start, segment.end, segment.text))\n",
    "    \n",
    "tac = time.time()\n",
    "print(\"Segmentation time: %f s for segmenting %.2f minutes audio\"%(tac - tic, total_audio_length/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04c773-a63d-4134-aae5-475ba0de9436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fbc282-be83-49b4-b462-ac2914a3404c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f775b8bb-d30e-499a-b094-8da60b51d7e2",
   "metadata": {},
   "source": [
    "## GPU Usage of faster-whisperFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed4b27-c1f2-4d91-b0d6-2ddafdce5eba",
   "metadata": {},
   "source": [
    "GPU usage when idle: 3.8 GB <br>\n",
    "GPU usage when segmenting (with a internal batch size 16):  up to 6 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdffe39-bcff-4beb-ac7f-4ea62faf0471",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
